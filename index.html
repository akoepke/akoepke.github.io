<!doctype html>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SQC9EK3HLV"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-SQC9EK3HLV");
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>A. Sophia Koepke</title>
    <meta name="author" content="A. Sophia Koepke" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <link rel="icon" href="images/tubingen_icon.jpg" />
  </head>
  <body>
    <nav>
      <div class="logo">
        <a href="#">A. Sophia Koepke</a>
      </div>
      <div id="hamburger">
        <div class="line"></div>
        <div class="line"></div>
        <div class="line"></div>
      </div>
      <ul id="nav-links">
        <li><a href="#">Home</a></li>
        <li><a href="mumol.html#news">News</a></li>
        <li><a href="#publications">Publications</a></li>
        <li>
          <a href="mumol.html">Team</a>
        </li>
      </ul>
    </nav>
    <div id="main-content">
      <table>
        <tr>
          <td style="padding: 2.5%; width: 63%">
            <p style="text-align: center">
              <name>Almut Sophia Koepke</name>
            </p>
            <div style="margin-bottom: 3.2em; margin-top: 3.2em">
              <p>
                I am a junior research group leader at the Technical University of Munich and the University of Tübingen.                 My research focusses on multi-modal learning problems with sound,
                vision, and text.
              </p>
              <p>
                I completed my DPhil in the Visual Geometry Group (<a href="https://www.robots.ox.ac.uk/~vgg/">VGG</a>)
                at the University of Oxford, supervised by
                <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>.
                I spent a summer at Reichman University working with <a href="https://faculty.runi.ac.il/moses/">Yael Moses</a>.
                Following my DPhil, I was a post-doctoral
                researcher with <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a> at the University of Tübingen. Furthermore, I was a visiting scholar at UC Berkeley, advised by <a href="https://people.eecs.berkeley.edu/~efros/">Alexei (Alyosha) Efros</a>. 
              </p>
            </div>
            <p class="bio-links">
              <a href="mailto:removeifyouarehuman-a-sophia.koepke@uni-tuebingen.de">Email</a>
              &nbsp;/&nbsp;
              <a href="https://www.linkedin.com/in/a-sophia-koepke/">LinkedIn</a>
              &nbsp;/&nbsp;
              <a href="https://x.com/ASophiaKoepke">Twitter</a>
              &nbsp;/&nbsp;
              <a href="https://bsky.app/profile/askoepke.bsky.social">Bluesky</a>
              &nbsp;/&nbsp;
              <a href="https://scholar.google.com/citations?user=q9zQhj8AAAAJ&">Google Scholar</a>
            </p>
          </td>
        </tr>
      </table>
      <table>
        <tr>
          <td style="padding: 20px; width: 100%">
            <h2 class="custom-heading" id="publications">Publications</h2>
          </td>
        </tr>
      </table>
      <table id="pubs">
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/vggsounder.jpg" alt="teaser-image" width="160" height="94" />
          </td>
          <td>
            <a href="#">
              <papertitle
                >VGGSounder: Audio-visual evaluations for foundation models</papertitle
              >
            </a>
            <br />
            <a href="https://akoepke.github.io/mumol.html">Daniil Zverev</a>*,
            <a href="https://scholar.google.com/citations?user=aeCiRSYAAAAJ&hl=en">Thaddäus Wiedemer</a>*,
            <a href="https://drimpossible.github.io/">Ameya Prabhu</a>,
            <a href="https://bethgelab.org/">Matthias Bethge</a>,
            <a href="https://is.mpg.de/person/wbrendel">Wieland Brendel</a>,
            <strong>A. Sophia Koepke</strong>
            <br />
            <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2025
            <br />
            <a href="#">paper (coming soon)</a>
            /
            <a href="#">code (coming soon)</a>
            <br />
            <span style="color: red; font-style: italic"
              >New benchmark for measuring modality contributions in audio-visual foundation models.</span
            >
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/acmmm.jpg" alt="teaser-image" width="160" height="78" />
          </td>
          <td>
            <a href="https://arxiv.org/abs/2409.00851">
              <papertitle>Dissecting temporal understanding in text-to-audio retrieval</papertitle>
            </a>
            <br />
            <a href="https://www.robots.ox.ac.uk/~oncescu/">Andreea-Maria Oncescu</a>,
            <a href="https://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>,
            <strong>A. Sophia Koepke</strong>
            <br />
            <em>ACM Multimedia (ACMMM)</em>, 2024
            <br />
            <a href="https://arxiv.org/abs/2409.00851">paper</a>
            /
            <a href="https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/dtu/">project page</a>
            /
            <a href="https://github.com/oncescuandreea/DTU_text_audio">code</a>
            <tr>
              <td colspan="2" style="height: 3px"></td>
            </tr>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/iclr_transfer.jpg" alt="teaser-image" width="160" height="96" />
          </td>
          <td>
            <a href="https://arxiv.org/abs/2310.17653">
              <papertitle
                >Fantastic gains and where to find them: On the existence and prospect of general knowledge transfer
                between any pretrained model</papertitle
              >
            </a>
            <br />
            <a href="https://karroth.com/">Karsten Roth</a>*,
            <a href="https://www.eml-unitue.de/people/lukas-thede">Lukas Thede</a>*, <strong>A. Sophia Koepke</strong>,
            <a href="https://research.google/people/OriolVinyals/">Oriol Vinyals</a>,
            <a href="https://www.olivierhenaff.com/">Olivier J. Hénaff</a>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>International Conference on Learning Representations (ICLR)</em>, 2024
            <br />
            <a href="https://arxiv.org/abs/2310.17653">paper</a>
            /
            <a href="https://github.com/ExplainableML/General-Knowledge-Transfer">code</a>
            /
            <a href="https://openreview.net/forum?id=m50eKHCttz">openreview</a>
            <br />
            <span style="color: red; font-style: italic">Spotlight.</span>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <td style="padding: 20px">
          <img src="images/clipclap.jpg" alt="teaser-image" width="160" height="88" />
        </td>
        <td>
          <a href="https://arxiv.org/pdf/2404.06309">
            <papertitle
              >Audio-visual generalized zero-shot learning using pre-trained large multi-modal models
            </papertitle>
          </a>
          <br />
          <a href="https://www.linkedin.com/in/david-kurzendoerfer-199501181/">David Kurzend&ouml;rfer</a>,
          <a href="https://merceaotniel.github.io/">Otniel-Bogdan Mercea</a>,
          <strong>A. Sophia Koepke</strong>,
          <a href="https://www.eml-munich.de/people/zeynep-akata">Zeynep Akata</a>
          <br />
          <em>Workshop on Learning with Limited Labelled Data for Image and Video Understanding at
          the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
          <br />
          <a href="https://arxiv.org/pdf/2404.06309">paper</a>
          /
          <a href="https://github.com/dkurzend/ClipClap-GZSL">code</a>
        </td>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <td style="padding: 20px">
          <img src="images/epic_icassp.jpg" alt="teaser-image" width="160" height="77" />
        </td>
        <td>
          <a href="https://arxiv.org/pdf/2402.19106v1.pdf">
            <papertitle
              >A sound approach: Using large language models to generate audio descriptions for egocentric text-audio
              retrieval
            </papertitle>
          </a>
          <br />
          <a href="https://www.robots.ox.ac.uk/~oncescu/">Andreea-Maria Oncescu</a>,
          <a href="https://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>,
          <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>,
          <a href="https://samuelalbanie.com/">Samuel Albanie</a>,
          <strong>A. Sophia Koepke</strong>
          <br />
          <em>The International Conference on Acoustics, Speech, & Signal Processing (ICASSP)</em>, 2024
          <br />
          <a href="https://arxiv.org/pdf/2402.19106v1.pdf">paper</a>
          /
          <a href="https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/ego/">project page</a>
          /
          <a href="https://github.com/oncescuandreea/audio_egovlp">code</a>
        </td>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/deep_graph_persistence.jpg" alt="teaser-image" width="160" height="65" />
          </td>
          <td>
            <a href="https://openreview.net/pdf?id=oyfRWeoUJY">
              <papertitle>Addressing caveats of neural persistence with deep graph persistence</papertitle>
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/leander-girrbach">Leander Girrbach</a>,
            <a href="https://www.eml-unitue.de/people/anders-christensen">Anders Christensen</a>,
            <a href="https://olewinther.github.io/">Ole Winther</a>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
            <strong>A. Sophia Koepke</strong>
            <br />
            <em>Transactions on Machine Learning Research (TMLR)</em>, 2023
            <br />
            <a href="https://arxiv.org/pdf/2307.10865.pdf">paper</a>
            /
            <a href="https://github.com/ExplainableML/Deep-Graph-Persistence">code</a>
            /
            <a href="https://openreview.net/forum?id=oyfRWeoUJY">openreview</a>
            /
            <a href="https://www.youtube.com/watch?v=KfCpoPYK_CY">video</a>
            <br />
            <span style="color: black; font-style: italic"
              >A part of this was also presented at the TAG-ML Workshop at ICML 2023.</span
            >
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/zeraucap.jpg" alt="teaser-image" width="160" height="83" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2311.08396.pdf">
              <papertitle
                >Zero-shot audio captioning with audio-language model guidance and audio context keywords</papertitle
              >
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/leonard-salewski">Leonard Salewski</a>, Stefan Fauth,
            <strong>A. Sophia Koepke</strong>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>NeurIPS Workshop on Machine Learning for Audio</em>, 2023
            <br />
            <a href="https://arxiv.org/pdf/2311.08396.pdf">paper</a>
            /
            <a href="https://github.com/ExplainableML/ZerAuCap">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/bmvc_2023.jpg" alt="teaser-image" width="160" height="76" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2309.15086.pdf">
              <papertitle>Video-adverb retrieval with compositional adverb-action embeddings</papertitle>
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/thomas-hummel">Thomas Hummel</a>,
            <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>,
            <strong>A. Sophia Koepke</strong>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>British Machine Vision Conference (BMVC)</em>, 2023
            <br />
            <a href="https://arxiv.org/pdf/2309.15086.pdf">paper</a>
            /
            <a href="https://hummelth.github.io/ReGaDa/index.html">project page</a>
            /
            <a href="https://github.com/ExplainableML/ReGaDa">code</a>
            <br />
            <span style="color: red; font-style: italic">Oral presentation.</span>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/waffling.jpg" alt="teaser-image" width="160" height="116" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2306.07282.pdf">
              <papertitle
                >Waffling around for performance: Visual classification with random words and broad concepts</papertitle
              >
            </a>
            <br />
            <a href="https://karroth.com/">Karsten Roth</a>,
            <a href="https://jaemyung-kim.github.io/">Jae Myung Kim</a>, <strong>A. Sophia Koepke</strong>,
            <a href="https://research.google/people/OriolVinyals/">Oriol Vinyals</a>,
            <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2023
            <br />
            <a href="https://arxiv.org/pdf/2306.07282.pdf">paper</a>
            /
            <a href="https://github.com/ExplainableML/WaffleCLIP">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/icis.jpg" alt="teaser-image" width="160" height="141" />
          </td>
          <td>
            <a href="#">
              <papertitle>Image-free classifier injection for zero-shot classification</papertitle>
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/anders-christensen">Anders Christensen</a>,
            <a href="https://mancinimassimiliano.github.io/">Massimiliano Mancini</a>,
            <strong>A. Sophia Koepke</strong>, <a href="https://olewinther.github.io/">Ole Winther</a>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2023
            <br />
            <a href="https://arxiv.org/pdf/2308.10599.pdf">paper</a>
            /
            <a href="https://github.com/ExplainableML/ImageFreeZSL">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/gcpr_av_2023.jpg" alt="teaser-image" width="160" height="128" />
          </td>
          <td>
            <a href="#">
              <papertitle>Text-to-feature diffusion for audio-visual few-shot learning</papertitle>
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>,
            <a href="https://www.eml-unitue.de/people/thomas-hummel">Thomas Hummel</a>,
            <strong>A. Sophia Koepke</strong>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>DAGM German Conference on Pattern Recognition (GCPR)</em>, 2023
            <br />
            <a href="https://arxiv.org/pdf/2309.03869.pdf">paper</a>
            /
            <a href="https://github.com/ExplainableML/AVDIFF-GFSL">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/gcpr.jpg" alt="teaser-image" width="160" height="82" />
          </td>
          <td>
            <a href="#">
              <papertitle>Zero-shot translation of attention patterns in VQA models to natural language</papertitle>
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/leonard-salewski">Leonard Salewski</a>,
            <strong>A. Sophia Koepke</strong>,
            <a
              href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/"
              >Hendrik Lensch</a
            >,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>DAGM German Conference on Pattern Recognition (GCPR)</em>, 2023
            <br />
            <a href="https://arxiv.org/pdf/2311.05043.pdf">paper</a>
            /
            <a href="https://github.com/ExplainableML/ZS-A2T">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/spurious.jpg" alt="teaser-image" width="160" height="145" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2304.03391.pdf">
              <papertitle>Exposing and mitigating spurious correlations for cross-modal retrieval</papertitle>
            </a>
            <br />
            <a href="https://jaemyung-kim.github.io/">Jae Myung Kim</a>, <strong>A. Sophia Koepke</strong>,
            <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em
              >Multimodal Learning and Applications Workshop at the IEEE/CVF Conference on Computer Vision and Pattern
              Recognition (CVPRW)</em
            >, 2023
            <br />
            <a href="https://arxiv.org/pdf/2304.03391.pdf">paper</a>
            /
            <a href="https://github.com/ExplainableML/Spurious_CM_Retrieval">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/plant.jpg" alt="teaser-image" width="160" height="77" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2210.14222.pdf">
              <papertitle>PlanT: Explainable planning transformers via object-level representations</papertitle>
            </a>
            <br />
            <a href="https://www.katrinrenz.de/">Katrin Renz</a>,
            <a href="https://kashyap7x.github.io/">Kashyap Chitta</a>,
            <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>,
            <strong>A. Sophia Koepke</strong>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
            <a href="https://www.cvlibs.net/">Andreas Geiger</a>
            <br />
            <em>Conference on Robot Learning (CoRL)</em>, 2022
            <br />
            <a href="https://arxiv.org/pdf/2210.14222.pdf">paper</a>
            /
            <a href="https://www.katrinrenz.de/plant">project page</a>
            /
            <a href="https://github.com/autonomousvision/plant">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/tcaf.jpg" alt="teaser-image" width="160" height="53" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2207.09966.pdf">
              <papertitle>Temporal and cross-modal attention for audio-visual zero-shot learning</papertitle>
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>*,
            <a href="https://www.eml-unitue.de/people/thomas-hummel">Thomas Hummel</a>*,
            <strong>A. Sophia Koepke</strong>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>European Conference on Computer Vision (ECCV)</em>, 2022
            <br />
            <a href="https://arxiv.org/pdf/2207.09966.pdf">paper</a>
            /
            <a href="https://github.com/ExplainableML/TCAF-GZSL">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/avca.jpg" alt="teaser-image" width="160" height="100" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2203.03598.pdf">
              <papertitle
                >Audio-visual generalised zero-shot learning with cross-modal attention and language</papertitle
              >
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>, Lukas Riesch,
            <strong>A. Sophia Koepke</strong>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
            <br />
            <a href="https://arxiv.org/pdf/2203.03598.pdf">paper</a>
            /
            <a href="https://github.com/ExplainableML/AVCA-GZSL">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/clevrx.jpg" alt="teaser-image" width="160" height="78" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2204.02380.pdf">
              <papertitle>CLEVR-X: A visual reasoning dataset for natural language explanations</papertitle>
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/leonard-salewski">Leonard Salewski</a>,
            <strong>A. Sophia Koepke</strong>,
            <a
              href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/"
              >Hendrik Lensch</a
            >,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>Springer Lecture Notes on Artificial Intelligence</em>, 2022
            <br />
            <a href="https://arxiv.org/pdf/2204.02380.pdf">paper</a>
            /
            <a href="https://explainableml.github.io/CLEVR-X/">project page</a>
            /
            <a href="https://github.com/ExplainableML/CLEVR-X">code</a>
            <br />
            <span style="color: black; font-style: italic"
              >This was also presented at the CVPR 2022 Workshop on Explainable AI for Computer Vision (XAI4CV).</span
            >
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/audio_benchmark.jpg" alt="teaser-image" width="160" height="131" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2112.09418.pdf">
              <papertitle>Audio retrieval with natural language queries: A benchmark study</papertitle>
            </a>
            <br />
            <strong>A. Sophia Koepke</strong>*,
            <a href="https://www.robots.ox.ac.uk/~oncescu/">Andreea-Maria Oncescu</a>*,
            <a href="https://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
            <a href="https://samuelalbanie.com/">Samuel Albanie</a>
            <br />
            <em>Transactions on Multimedia</em>, 2022
            <br />
            <a href="https://arxiv.org/pdf/2112.09418.pdf">paper</a>
            /
            <a href="https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/">project page</a>
            /
            <a href="https://github.com/akoepke/audio-retrieval-benchmark">code</a>
            <br />
            <span style="color: red; font-style: italic"
              >Extension of the INTERSPEECH paper with a new dataset and new results.</span
            >
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/audio_retrieval.jpg" alt="teaser-image" width="160" height="109" />
          </td>
          <td>
            <a href="https://arxiv.org/pdf/2105.02192.pdf">
              <papertitle>Audio retrieval with natural language queries</papertitle>
            </a>
            <br />
            <a href="https://www.robots.ox.ac.uk/~oncescu/">Andreea-Maria Oncescu</a>*,
            <strong>A. Sophia Koepke</strong>*, <a href="https://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
            <a href="https://samuelalbanie.com/">Samuel Albanie</a>
            <br />
            <em>INTERSPEECH</em>, 2021
            <br />
            <a href="https://arxiv.org/pdf/2105.02192.pdf">paper</a>
            /
            <a href="https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/">project page</a>
            /
            <a href="https://github.com/oncescuandreea/audio-retrieval">code</a>
            <br />
            <span style="color: red; font-style: italic">Shortlisted for best student paper award.</span>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/distilling.jpg" alt="teaser-image" width="160" height="92" />
          </td>
          <td>
            <a href="https://arxiv.org/abs/2104.10955">
              <papertitle>Distilling audio-visual knowledge by compositional contrastive learning</papertitle>
            </a>
            <br />
            <a href="https://www.eml-unitue.de/people/yanbei-chen">Yanbei Chen</a>,
            <a href="https://www.eml-unitue.de/people/yongqin-xian">Yongqin Xian</a>, <strong>A. Sophia Koepke</strong>,
            Ying Shan,
            <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
            <br />
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
            <br />
            <a href="https://arxiv.org/abs/2104.10955">paper</a>
            /
            <a href="https://github.com/yanbeic/CCL">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/sight_to_sound.jpg" alt="teaser-image" width="160" height="88" />
          </td>
          <td>
            <a href="https://www.robots.ox.ac.uk/~vgg/publications/2020/Koepke20/koepke20.pdf">
              <papertitle>Sight to sound: An end-to-end approach for visual piano transcription</papertitle>
            </a>
            <br />
            <strong>A. Sophia Koepke</strong>, <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>,
            <a href="https://faculty.idc.ac.il/moses/">Yael Moses</a>,
            <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
            <br />
            <em>The International Conference on Acoustics, Speech, & Signal Processing (ICASSP)</em>, 2020
            <br />
            <a href="https://www.robots.ox.ac.uk/~vgg/publications/2020/Koepke20/koepke20.pdf">paper</a>
            /
            <a href="https://www.robots.ox.ac.uk/~vgg/research/sighttosound/">project page</a>
            <br />
            <span style="color: red; font-style: italic">Oral presentation.</span>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/self_supervised_embedding.jpg" alt="teaser-image" width="160" height="90" />
          </td>
          <td>
            <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Wiles19/wiles19.pdf">
              <papertitle>Self-supervised learning of class embeddings from video</papertitle>
            </a>
            <br />
            <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>, <strong>A. Sophia Koepke</strong>,
            <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
            <br />
            <em>
              Compact and Efficient Feature Representation and Learning in Computer Vision Workshop at the IEEE/CVF
              International Conference on Computer Vision (ICCV Workshop)</em
            >, 2019
            <br />
            <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Wiles19/wiles19.pdf">paper</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr>
          <td style="padding: 20px">
            <img src="images/visual_pitch_estimation.jpg" alt="teaser-image" width="160" height="64" />
          </td>
          <td>
            <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Koepke19/koepke19.pdf">
              <papertitle>Visual pitch estimation</papertitle>
            </a>
            <br />
            <strong>A. Sophia Koepke</strong>, <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>,
            <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
            <br />
            <em>Sound and Music Computation Conference (SMC)</em>, 2019
            <br />
            <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Koepke19/koepke19.pdf">paper</a>
            /
            <a href="https://www.robots.ox.ac.uk/~vgg/research/sighttosound/violinpitch.html">project page</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/facial_attribute_embedding.jpg" alt="teaser-image" width="160" height="92" />
          </td>
          <td>
            <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Wiles18a/wiles18a.pdf">
              <papertitle>Self-supervised learning of a facial attribute embedding from video</papertitle>
            </a>
            <br />
            <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>*, <strong>A. Sophia Koepke</strong>*,
            <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
            <br />
            <em>British Machine Vision Conference (BMVC)</em>, 2018
            <br />
            <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Wiles18a/wiles18a.pdf">paper</a>
            /
            <a href="http://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/resources/wiles18a_supp.pdf"
              >supplementary material</a
            >
            /
            <a href="https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/fabnet.html">project page</a>
            /
            <a href="https://github.com/oawiles/FAb-Net">code</a>
            <br />
            <span style="color: red; font-style: italic">Oral presentation.</span>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td colspan="2" style="padding: 3px; border-top: 1px solid #ccc"></td>
        </tr>
        <tr class="featured">
          <td style="padding: 20px">
            <img src="images/x2face.jpg" alt="teaser-image" width="160" height="90" />
          </td>
          <td>
            <a href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Wiles18/wiles18.pdf">
              <papertitle
                >X2Face: A network for controlling face generation by using images, audio, and pose codes</papertitle
              >
            </a>
            <br />
            <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>*, <strong>A. Sophia Koepke</strong>*,
            <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
            <br />
            <em>European Conference on Computer Vision (ECCV)</em>, 2018
            <br />
            <a href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Wiles18/wiles18.pdf">paper</a>
            /
            <a href="https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html">project page</a>
            /
            <a href="https://github.com/oawiles/X2Face">code</a>
          </td>
        </tr>
        <tr>
          <td colspan="2" style="height: 3px"></td>
        </tr>
        <tr>
          <td style="text-align: left; padding-left: 20px; padding-bottom: 30px">* denotes equal contribution</td>
        </tr>
      </table>
      <table>
        <tr>
          <td style="padding: 20px; width: 100%">
            <heading>Teaching</heading>
            <p>
              <a href="https://www.eml-unitue.de/teaching/lectures"
                >Introduction to Machine Learning (University of Tübingen, Summer semester 2023)</a
              >
            </p>
          </td>
        </tr>
      </table>
      <table>
        <tr>
          <td style="padding: 20px; width: 100%">
            <heading>Community service</heading>
            <p>Area chair:</p>

            <ul>
              <li><a href="https://bmvc2025.bmva.org/">BMVC 2025</a></li>
              <li><a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a></li>
              <li><a href="https://bmvc2024.org/">BMVC 2024</a></li>
              <li>
                <a href="https://eccv.ecva.net/Conferences/2024/AreaChairs">ECCV 2024</a>
              </li>
              <li>
                <a href="https://bmvc2023.org/people/area-chairs/">BMVC 2023</a>
              </li>
              <li>
                <a href="https://bmvc2022.org/people/area-chairs/">BMVC 2022</a>
              </li>
            </ul>
            <br />
            Outstanding reviewer:
            <ul>
              <li>
                <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">CVPR 2022</a>
              </li>
              <li>
                <a href="https://eccv2022.ecva.net/program/outstanding-reviewers/">ECCV 2022</a>
              </li>
              <li>
                <a href="https://twitter.com/accv2020/status/1333932083777978368/photo/1">ACCV 2020</a>
              </li>
            </ul>
            <br />
            Reviewer:
            <ul>
              <li>ICCV (2023, 2025)</li>
              <li>NeurIPS (2023)</li>
              <li>ECCV (2022)</li>
              <li>CVPR (2021, 2022)</li>
              <li>ACCV (2020)</li>
              <li>CVPR workshop: Learning with Limited Labelled Data for Image and Video Understanding (2022)</li>
              <li>ICCV workshop: Closing the loop between Vision and Language (2021)</li>
              <li>
                NeurIPS workshop: The preregistration experiment: an alternative publication model for machine learning
                research (2020)
              </li>
              <li>CVPR workshop: Women in Computer Vision (2019, 2020)</li>
              <li>ICCV workshop: Neural Architects (2019)</li>
              <li>IJCV</li>
              <li>TPAMI</li>
              <li>IEEE Access</li>
              <li>Eurographics</li>
            </ul>
            <br />
            Workshop organisation:
            <ul>
              <li>
                <a href="https://sites.google.com/view/eval-fomo-2-cvpr/home"
                  >2nd Workshop on Emergent Visual Abilities and Limits of Foundation Models (CVPR, 2025)</a
                >
              </li>
              <li>
                <a href="https://sites.google.com/view/eval-fomo-24/home"
                  >1st Workshop on Emergent Visual Abilities and Limits of Foundation Models (ECCV, 2024)</a
                >
              </li>
              <li>
                <a href="https://www.eml-unitue.de/events/external_events"
                  >Explainability in Machine Learning (Tübingen, 2023)</a
                >
              </li>
              <li>
                <a href="https://sites.google.com/view/imagenet-workshop/"
                  >ImageNet: Past, Present, and Future (NeurIPS, 2021)</a
                >
              </li>
            </ul>
          </td>
        </tr>
      </table>
      <table>
        <tr>
          <td style="padding: 20px; width: 100%">
            <heading>Misc</heading>
            <p>
              <a href="https://tuebingen.ai/">Tübingen AI Center</a>
              <br />
              <a href="https://mcml.ai/">Munich Center for Machine Learning (MCML)</a>
              <br />
              <a href="https://ellis.eu">ELLIS Society</a>
              <br />
              <a href="https://ki-macht-schule.de/">KI macht Schule</a>
              (AI education for pupils)
            </p>
          </td>
        </tr>
      </table>
    </div>
    <footer>
      <p>
        Website template adapted from
        <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
      </p>
    </footer>
    <script src="script.js"></script>
  </body>
</html>
