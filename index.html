<!DOCTYPE HTML>
<html lang="en">
<head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W4NT1G885L"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W4NT1G885L');
</script>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>A. Sophia Koepke</title>
  <meta name="author" content="A. Sophia Koepke">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/tubingen_icon.jpg">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Almut Sophia Koepke</name>
              </p>
              <p>I am a post-doctoral research fellow in the <a href="https://www.eml-unitue.de/">Explainable Machine Learning group</a> at the University of T&uuml;bingen (Germany), where I work with <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>. My research focusses on multi-modal learning problems with sound, vision, and language.
              </p>
              <p>
                I completed my DPhil in the <a href="https://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group (VGG)</a> at the University of Oxford, supervised by <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>. I spent a summer at Reichman University working with <a href="https://faculty.runi.ac.il/moses/">Yael Moses</a>.
              Before that, I obtained a MSc in Mathematical Modelling and Scientific Computing at the University of Oxford, and a BSc in Mathematics at the University of Freiburg (Germany).
              </p>
              <br>
              <p style="text-align:center">
                <a href="mailto:removeifyouarehuman-a-sophia.koepke@uni-tuebingen.de">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/a-sophia-koepke/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q9zQhj8AAAAJ&hl=de">Google Scholar</a> &nbsp
              </p>
              <br>
                <b>I am happy to collaborate and to supervise students at the University of T&uuml;bingen for BSc and MSc projects. Feel free to get in touch via email if you are interested in working together.</b>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
<tr bgcolor="#fffebbe">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bmvc_2023.jpg" alt="teaser-image" width="160" height="76">
            </td>
            <td width="75%" valign="middle">
              <a href="#">
                <papertitle>Video-adverb retrieval with compositional adverb-action embeddings</papertitle>
              </a>
              <br>
              <a href="https://www.eml-unitue.de/people/thomas-hummel">Thomas Hummel</a>, <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>, <strong>A. Sophia Koepke</strong>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2023
              <br>
    <a href="https://hummelth.github.io/ReGaDa/index.html">project page</a>
    /
    <a href="https://github.com/ExplainableML/ReGaDa">code</a>
    <br>
    <span style="color:red; font-style:italic">Oral presentation.</span>
            </td>
          </tr>

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>

					
          <tr bgcolor="#fffebbe">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/caveats.jpg" alt="teaser-image" width="160" height="78">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=YJ6Y7ucAxi">
                <papertitle>Caveats of neural persistence in deep neural networks</papertitle>
              </a>
              <br>
              <a href="https://www.eml-unitue.de/people/leander-girrbach">Leander Girrbach</a>, <a href="https://www.eml-unitue.de/people/anders-christensen">Anders Christensen</a>, <a href="https://olewinther.github.io/">Ole Winther</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>, <strong>A. Sophia Koepke</strong> 
              <br>
              <em>Topology, Algebra, and Geometry in Machine Learning Workshop (TAG-ML) at International Conference on Machine Learning (ICMLW)</em>, 2023
              <br>
        <a href="https://openreview.net/pdf?id=YJ6Y7ucAxi">extended abstract</a>
            </td>
          </tr>
<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
          <tr bgcolor="#fffebbe">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/waffling.jpg" alt="teaser-image" width="160" height="116">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2306.07282.pdf">
                <papertitle>Waffling around for performance: Visual classification with random words and broad concepts</papertitle>
              </a>
              <br>
              <a href="https://karroth.com/">Karsten Roth</a>, <a href="https://jaemyung-kim.github.io/">Jae Myung Kim</a>, <strong>A. Sophia Koepke</strong>, <a href="https://research.google/people/OriolVinyals/">Oriol Vinyals</a>, <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
              <br>
              <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2023
              <br>
        <a href="https://arxiv.org/pdf/2306.07282.pdf">paper</a>
        /
        <a href="https://github.com/ExplainableML/WaffleCLIP">code</a>
            </td>
          </tr>
        
<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
     
        <tr bgcolor="#fffebbe">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/icis.jpg" alt="teaser-image" width="160" height="141">
            </td>
            <td width="75%" valign="middle">
              <a href="#">
                <papertitle>Image-free classifier injection for zero-shot classification</papertitle>
              </a>
              <br>
              <a href="https://www.eml-unitue.de/people/anders-christensen">Anders Christensen</a>, <a href="https://mancinimassimiliano.github.io/">Massimiliano Mancini</a>, <strong>A. Sophia Koepke</strong>, <a href="https://olewinther.github.io/">Ole Winther</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
              <br>
              <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2023
              <br>
        <a href="https://arxiv.org/pdf/2308.10599.pdf">paper</a>
      /
      <a href="https://github.com/ExplainableML/ImageFreeZSL">code</a>
            </td>
          </tr>
       

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>

<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gcpr_av_2023.jpg" alt="teaser-image" width="160" height="128">
            </td>
            <td width="75%" valign="middle">
              <a href="#">
                <papertitle>Text-to-feature diffusion for audio-visual few-shot learning</papertitle>
              </a>
              <br>
              <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>, <a href="https://www.eml-unitue.de/people/thomas-hummel">Thomas Hummel</a>, <strong>A. Sophia Koepke</strong>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
              <br>
              <em>DAGM German Conference on Pattern Recognition (GCPR)</em>, 2023
              <br>
        <a href="https://arxiv.org/pdf/2309.03869.pdf">paper</a>
      /
      <a href="https://github.com/ExplainableML/AVDIFF-GFSL">code</a>
            </td>
          </tr>


<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>

      <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gcpr.jpg" alt="teaser-image" width="160" height="82">
            </td>
            <td width="75%" valign="middle">
              <a href="#">
                <papertitle>Zero-shot translation of attention patterns in VQA models to natural language</papertitle>
              </a>
              <br>
              <a href="https://www.eml-unitue.de/people/leonard-salewski">Leonard Salewski</a>, <strong>A. Sophia Koepke</strong>, <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/">Hendrik Lensch</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
              <br>
              <em>DAGM German Conference on Pattern Recognition (GCPR)</em>, 2023
              <br>
            </td>
          </tr>
        
<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/spurious.jpg" alt="teaser-image" width="160" height="145">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2304.03391.pdf">
                <papertitle>Exposing and mitigating spurious correlations for cross-modal retrieval</papertitle>
              </a>
              <br>
              <a href="https://jaemyung-kim.github.io/">Jae Myung Kim</a>, <strong>A. Sophia Koepke</strong>, <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
              <br>
              <em>Multimodal Learning and Applications Workshop at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPRW)</em>, 2023
              <br>
        <a href="https://arxiv.org/pdf/2304.03391.pdf">paper</a>
        /
        <a href="https://github.com/ExplainableML/Spurious_CM_Retrieval">code</a>
            </td>
          </tr>
       
<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>

  <tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/plant.jpg" alt="teaser-image" width="160" height=77">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2210.14222.pdf">
      <papertitle>PlanT: Explainable planning transformers via object-level representations</papertitle>
    </a>
    <br>
    <a href="https://www.katrinrenz.de/">Katrin Renz</a>, <a href="https://kashyap7x.github.io/">Kashyap Chitta</a>, <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>, <strong>A. Sophia Koepke</strong>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>, <a href="https://www.cvlibs.net/">Andreas Geiger</a>
    <br>
    <em>Conference on Robot Learning (CoRL)</em>, 2022
    <br>
    <a href="https://arxiv.org/pdf/2210.14222.pdf">paper</a>
    /
    <a href="https://www.katrinrenz.de/plant">project page</a>
    /
    <a href="https://github.com/autonomousvision/plant">code</a>
  </td>
</tr>


<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>

<tr bgcolor="#fffebbe">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/tcaf.jpg" alt="teaser-image" width="160" height="53">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2207.09966.pdf">
      <papertitle>Temporal and cross-modal attention for audio-visual zero-shot learning</papertitle>
    </a>
    <br>
    <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>*, <a href="https://www.eml-unitue.de/people/thomas-hummel">Thomas Hummel</a>*, <strong>A. Sophia Koepke</strong>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
    <br>
    <em>European Conference on Computer Vision (ECCV)</em>, 2022
    <br>
    <a href="https://arxiv.org/pdf/2207.09966.pdf">paper</a>
    /
    <a href="https://github.com/ExplainableML/TCAF-GZSL">code</a>
  </td>
</tr>


<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
<tr bgcolor="#fffebbe">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/avca.jpg" alt="teaser-image" width="160" height="100">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2203.03598.pdf">
      <papertitle>Audio-visual generalised zero-shot learning with cross-modal attention and language</papertitle>
    </a>
    <br>
    <a href="https://www.eml-unitue.de/people/otniel-mercea">Otniel-Bogdan Mercea</a>, Lukas Riesch, <strong>A. Sophia Koepke</strong>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
    <br>
    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
    <br>
    <a href="https://arxiv.org/pdf/2203.03598.pdf">paper</a>
    /
    <a href="https://github.com/ExplainableML/AVCA-GZSL">code</a>
  </td>
</tr>

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/clevrx.jpg" alt="teaser-image" width="160" height="78">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2204.02380.pdf">
      <papertitle>CLEVR-X: A visual reasoning dataset for natural language explanations</papertitle>
    </a>
    <br>
    <a href="https://www.eml-unitue.de/people/leonard-salewski">Leonard Salewski</a>, <strong>A. Sophia Koepke</strong>, <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/">Hendrik Lensch</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
    <br>
    <em>Springer Lecture Notes on Artificial Intelligence</em>, 2022
    <br>
    <a href="https://arxiv.org/pdf/2204.02380.pdf">paper</a>
    /
    <a href="https://explainableml.github.io/CLEVR-X/">project page</a>
    /
    <a href="https://github.com/ExplainableML/CLEVR-X">code</a>
    <br>
    <span style="color:black; font-style:italic">This was also presented at the CVPR 2022 Workshop on Explainable AI for Computer Vision (XAI4CV).</span>
  </td>
</tr>

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>

<tr bgcolor="#fffebbe">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/audio_benchmark.jpg" alt="teaser-image" width="160" height="131">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2112.09418.pdf">
      <papertitle>Audio retrieval with natural language queries: A benchmark study</papertitle>
    </a>
    <br>
    <strong>A. Sophia Koepke</strong>*, <a href="https://www.robots.ox.ac.uk/~oncescu/">Andreea-Maria Oncescu</a>*, <a href="https://www.robots.ox.ac.uk/~joao/">Jo&atilde;o F. Henriques</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>, <a href="https://samuelalbanie.com/">Samuel Albanie</a>
    <br>
    <em>Transactions on Multimedia</em>, 2022
    <br>
    <a href="https://arxiv.org/pdf/2112.09418.pdf">paper</a>
    /
    <a href="https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/">project page</a>
    /
    <a href="https://github.com/akoepke/audio-retrieval-benchmark">code</a>
    <br>
    <span style="color:red; font-style:italic">Extension of the INTERSPEECH paper with a new dataset and new results.</span>
  </td>
</tr>

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
<tr bgcolor="#fffebbe">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/audio_retrieval.jpg" alt="teaser-image" width="160" height="109">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2105.02192.pdf">
      <papertitle>Audio retrieval with natural language queries</papertitle>
    </a>
    <br>
    <a href="https://www.robots.ox.ac.uk/~oncescu/">Andreea-Maria Oncescu</a>*, <strong>A. Sophia Koepke</strong>*, <a href="https://www.robots.ox.ac.uk/~joao/">Jo&atilde;o F. Henriques</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>, <a href="https://samuelalbanie.com/">Samuel Albanie</a>
    <br>
    <em>INTERSPEECH</em>, 2021
    <br>
    <a href="https://arxiv.org/pdf/2105.02192.pdf">paper</a>
    /
    <a href="https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/">project page</a>
    /
    <a href="https://github.com/oncescuandreea/audio-retrieval">code</a>
    <br>
    <span style="color:red; font-style:italic">Shortlisted for best student paper award.</span>
  </td>
</tr>

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/distilling.jpg" alt="teaser-image" width="160" height="92">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2104.10955">
      <papertitle>Distilling audio-visual knowledge by compositional contrastive learning</papertitle>
    </a>
    <br>
    <a href="https://www.eml-unitue.de/people/yanbei-chen">Yanbei Chen</a>, <a href="https://www.eml-unitue.de/people/yongqin-xian">Yongqin Xian</a>, <strong>A. Sophia Koepke</strong>, Ying Shan, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>
    <br>
    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
    <br>
    <a href="https://arxiv.org/abs/2104.10955">paper</a>
    /
    <a href="https://github.com/yanbeic/CCL">code</a>
  </td>
</tr>

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
<tr bgcolor="#fffebbe">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/sight_to_sound.jpg" alt="teaser-image" width="160" height="88">
  </td>
  <td width="75%" valign="middle">
    <a href="https://www.robots.ox.ac.uk/~vgg/publications/2020/Koepke20/koepke20.pdf">
      <papertitle>Sight to sound: An end-to-end approach for visual piano transcription</papertitle>
    </a>
    <br>
    <strong>A. Sophia Koepke</strong>, <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>, <a href="https://faculty.idc.ac.il/moses/">Yael Moses</a>, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
    <br>
    <em>The International Conference on Acoustics, Speech, & Signal Processing (ICASSP)</em>, 2020
    <br>
    <a href="https://www.robots.ox.ac.uk/~vgg/publications/2020/Koepke20/koepke20.pdf">paper</a>
    /
    <a href="https://www.robots.ox.ac.uk/~vgg/research/sighttosound/">project page</a>
    <br>
    <span style="color:red; font-style:italic">Oral presentation.</span>
  </td>
</tr>

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/self_supervised_embedding.jpg" alt="teaser-image" width="160" height="90">
  </td>
  <td width="75%" valign="middle">
    <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Wiles19/wiles19.pdf">
      <papertitle>Self-supervised learning of class embeddings from video</papertitle>
    </a>
    <br>
    <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>, <strong>A. Sophia Koepke</strong>, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
    <br>
    <em> Compact and Efficient Feature Representation and Learning in Computer Vision Workshop at the IEEE/CVF International Conference on Computer Vision (ICCV Workshop)</em>, 2019
    <br>
    <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Wiles19/wiles19.pdf">paper</a>
  </td>
</tr>

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/visual_pitch_estimation.jpg" alt="teaser-image" width="160" height="64">
  </td>
  <td width="75%" valign="middle">
    <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Koepke19/koepke19.pdf">
      <papertitle>Visual pitch estimation</papertitle>
    </a>
    <br>
    <strong>A. Sophia Koepke</strong>, <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
    <br>
    <em>Sound and Music Computation Conference (SMC)</em>, 2019
    <br>
    <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Koepke19/koepke19.pdf">paper</a>
    /
    <a href="https://www.robots.ox.ac.uk/~vgg/research/sighttosound/violinpitch.html">project page</a>
  </td>
</tr>

<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
<tr bgcolor="#fffebbe">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/facial_attribute_embedding.jpg" alt="teaser-image" width="160" height="92">
  </td>
  <td width="75%" valign="middle">
    <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Wiles18a/wiles18a.pdf">
      <papertitle>Self-supervised learning of a facial attribute embedding from video</papertitle>
    </a>
    <br>
    <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>*, <strong>A. Sophia Koepke</strong>*, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
    <br>
    <em>British Machine Vision Conference (BMVC)</em>, 2018
    <br>
    <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Wiles18a/wiles18a.pdf">paper</a>
    /
    <a href="http://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/resources/wiles18a_supp.pdf">supplementary material</a>
    /
    <a href="https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/fabnet.html">project page</a>
    /
    <a href="https://github.com/oawiles/FAb-Net">code</a>
    <br>
    <span style="color:red; font-style:italic">Oral presentation.</span>
  </td>
</tr>


<tr>
  <td colspan="2" style="padding:3px;border-top: 1px solid #ccc;"></td>
</tr>
<tr bgcolor="#fffebbe">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/x2face.jpg" alt="teaser-image" width="160" height="90">
  </td>
    <td width="75%" valign="middle">
      <a href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Wiles18/wiles18.pdf">
        <papertitle>X2Face: A network for controlling face generation by using images, audio, and pose codes</papertitle>
      </a>
      <br>
      <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>*, <strong>A. Sophia Koepke</strong>*, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
      <br>
      <em>European Conference on Computer Vision (ECCV)</em>, 2018
      <br>
      <a href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Wiles18/wiles18.pdf">paper</a>
      /
      <a href="https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html">project page</a>
      /
      <a href="https://github.com/oawiles/X2Face">code</a>
    </td>
  </tr>
</tr>


<tr>
  <td style="text-align: center; padding-left:20px; padding-bottom:30px;">
    * denotes equal contribution
  </td>
</tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p>
              <a href="https://www.eml-unitue.de/teaching/lectures">Introduction to Machine Learning (University of T&#252;bingen, Summer semester 2023)</a>
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Community service</heading>
              <p>
                Area chair:
                <ul>
                  <li>BMVC 2023</li>
                  <li><a href="https://bmvc2022.org/people/area-chairs/">BMVC 2022</a></li>
                </ul>
                <br>
                Outstanding reviewer:
                <ul>
                  <li><a href="https://cvpr2022.thecvf.com/outstanding-reviewers">CVPR 2022</a></li>
                  <li><a href="https://eccv2022.ecva.net/program/outstanding-reviewers/">ECCV 2022</a></li>
                  <li><a href="https://twitter.com/accv2020/status/1333932083777978368/photo/1">ACCV 2020</a></li>
                </ul>
                <br>
                Reviewer:
                <ul>
                  <li>CVPR (2021, 2022) </li>
                  <li>ECCV (2022) </li>
                  <li>ICCV (2023) </li>
                  <li>NeurIPS (2023) </li>
                  <li>ACCV (2020) </li>
                  <li>CVPR workshop: Learning with Limited Labelled Data for Image and Video Understanding (2022)</li>
                  <li>ICCV workshop: Closing the loop between Vision and Language (2021)</li>
                  <li>NeurIPS workshop: The preregistration experiment: an alternative publication model for machine learning research (2020)</li>
                  <li>CVPR workshop: Women in Computer Vision (2019, 2020)</li>
                  <li>ICCV workshop: Neural Architects (2019)</li>
                  <li>IJCV </li>
                  <li>TPAMI </li> 
                  <li>IEEE Access </li> 
                  <li>Eurographics </li> 
                </ul>
                <br>
                Workshop organisation:
                <ul>
                  <li><a href="https://www.eml-unitue.de/events/external_events">Explainability in Machine Learning (T&#252;bingen, 2023)</a></li> 
                  <li><a href="https://sites.google.com/view/imagenet-workshop/">ImageNet: Past, Present, and Future (NeurIPS, 2021)</a></li> 
                </ul>

              </p>
            </td>
          </tr>
        </tbody></table>
       

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Misc</heading>
              <p>
              <a href="https://tuebingen.ai/">TÃ¼bingen AI Center </a>
              <br>
              <a href="https://ki-macht-schule.de/">KI macht Schule </a> (AI education for pupils)
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template adapted from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
